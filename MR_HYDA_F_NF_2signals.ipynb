{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812f927e-cc28-4494-9a46-5edc409d5b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 10240, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing the files\n",
    "directory = r'E:\\sahithi\\FOCAL'  # Replace with your actual directory path\n",
    "\n",
    "# Initialize an empty array to store the data\n",
    "data_array = np.empty((3750, 10240, 2))  # Assuming 3750 files, each with 10240 time points and 2 columns\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for i, filename in enumerate(os.listdir(directory)):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read data from file using pandas, load both columns (assuming they are separated by commas)\n",
    "        data_df = pd.read_csv(file_path, delimiter=',', usecols=[0, 1], header=None)\n",
    "        \n",
    "        # Convert the data to numpy array\n",
    "        data = data_df.values\n",
    "        \n",
    "        # Ensure the data shape matches expectations\n",
    "        assert data.shape == (10240, 2), f\"File {filename} has unexpected shape {data.shape}\"\n",
    "        \n",
    "        # Store data in the array\n",
    "        data_array[i, :, :] = data\n",
    "\n",
    "# Print the shape of the final array\n",
    "print(data_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c90753-8729-4ff6-a990-f191c5644289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 10240, 2)\n"
     ]
    }
   ],
   "source": [
    "# Find rows that contain any NaN values\n",
    "nan_rows = np.any(np.isnan(data_array), axis=1)  # Rows containing any NaN values\n",
    "\n",
    "# Get the indices of rows with NaN values\n",
    "nan_row_indices = np.where(nan_rows)[0]\n",
    "\n",
    "# Delete the rows that contain NaN values\n",
    "focal_array = np.delete(data_array, nan_row_indices, axis=0)\n",
    "\n",
    "# Print the shape of the resulting array\n",
    "print(focal_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879d8152-c165-47dd-a10e-221b47d61c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_row_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3b1838-7fed-4494-b819-ff22e8d8694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 10240, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing the files\n",
    "directory = r'E:\\sahithi\\NON_FOCAL'  # Replace with your actual directory path\n",
    "\n",
    "# Initialize an empty array to store the data\n",
    "data_arrayy = np.empty((3750, 10240, 2))  # Assuming 3750 files, each with 10240 time points and 2 columns\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for i, filename in enumerate(os.listdir(directory)):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read data from file using pandas, load both columns (assuming they are separated by commas)\n",
    "        data_df = pd.read_csv(file_path, delimiter=',', usecols=[0, 1], header=None)\n",
    "        \n",
    "        # Convert the data to numpy array\n",
    "        data = data_df.values\n",
    "        \n",
    "        # Ensure the data shape matches expectations\n",
    "        assert data.shape == (10240, 2), f\"File {filename} has unexpected shape {data.shape}\"\n",
    "        \n",
    "        # Store data in the array\n",
    "        data_arrayy[i, :, :] = data\n",
    "\n",
    "# Print the shape of the final array\n",
    "print(data_arrayy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c604efa-0a48-4a75-a875-01a789908cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 10240, 2)\n"
     ]
    }
   ],
   "source": [
    "# Find rows that contain any NaN values\n",
    "nan_rows = np.any(np.isnan(data_arrayy), axis=1)  # Rows containing any NaN values\n",
    "\n",
    "# Get the indices of rows with NaN values\n",
    "nan_row_indices = np.where(nan_rows)[0]\n",
    "\n",
    "# Delete the rows that contain NaN values\n",
    "non_focal_array = np.delete(data_arrayy, nan_row_indices, axis=0)\n",
    "\n",
    "# Print the shape of the resulting array\n",
    "print(non_focal_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a5435ba-7492-462b-b3b6-fd0423733389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an array with zeros and ones\n",
    "y_nf = np.ones(3750)\n",
    "print(y_nf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f8fbda9-f211-4bab-980e-1df19a049b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an array with zeros and ones\n",
    "y_f = np.zeros(3750)\n",
    "print(y_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea88b927-4069-452c-a928-04726a33b0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked array horizontally:\n",
      "[[[-23.584467  22.621477]\n",
      "  [-20.180197  26.547081]\n",
      "  [-16.631811  28.431639]\n",
      "  ...\n",
      "  [ 17.817694  -4.722577]\n",
      "  [ 17.604982  -3.035015]\n",
      "  [ 15.054106  -2.973314]]\n",
      "\n",
      " [[  1.770672   2.16606 ]\n",
      "  [  7.448948  11.830342]\n",
      "  [ 13.643579  22.593559]\n",
      "  ...\n",
      "  [-12.643819 -14.031724]\n",
      "  [-11.053504 -20.490677]\n",
      "  [-11.453481 -23.759109]]\n",
      "\n",
      " [[-22.218569  -6.315594]\n",
      "  [-20.903902   2.320192]\n",
      "  [-22.920767   8.273545]\n",
      "  ...\n",
      "  [-41.046448 -12.789649]\n",
      "  [-42.06147  -15.303322]\n",
      "  [-46.215527 -21.942698]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-16.280914 -10.494535]\n",
      "  [-14.578059  -6.535292]\n",
      "  [-12.626638  -4.157788]\n",
      "  ...\n",
      "  [-21.634005  -9.552231]\n",
      "  [-22.678474 -10.085828]\n",
      "  [-22.286791  -9.175525]]\n",
      "\n",
      " [[ 11.63331   -1.116699]\n",
      "  [  8.432248  -9.232866]\n",
      "  [ -5.501138 -22.984268]\n",
      "  ...\n",
      "  [ 29.681234 -21.715002]\n",
      "  [ 28.138624 -61.248924]\n",
      "  [ 22.855257 -78.836479]]\n",
      "\n",
      " [[-21.539114 -24.517914]\n",
      "  [-21.258053 -26.164896]\n",
      "  [-23.109068 -29.641247]\n",
      "  ...\n",
      "  [ -1.207698  -4.656437]\n",
      "  [ -2.204289  -4.593308]\n",
      "  [ -3.187739  -4.21675 ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have two arrays\n",
    "array1 = focal_array\n",
    "array2 = non_focal_array\n",
    "\n",
    "# Stack them horizontally\n",
    "stacked_array = np.vstack((array1, array2))\n",
    "\n",
    "print(\"Stacked array horizontally:\")\n",
    "print(stacked_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe347ef7-5f19-4fb0-8b25-7cd1e22ba967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 10240, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f665fe23-2459-48be-b308-a3f76971e3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked array horizontally:\n",
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have two arrays\n",
    "array1 = y_f\n",
    "array2 = y_nf\n",
    "\n",
    "# Stack them horizontally\n",
    "y_labels = np.hstack((array1, array2))\n",
    "\n",
    "print(\"Stacked array horizontally:\")\n",
    "print(y_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87c12bfe-a136-44e5-842d-9f4907441ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500,)\n"
     ]
    }
   ],
   "source": [
    "print(y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1faafa1-0c71-462f-8cbb-eaca3e484682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels[3750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c42e2f-d8d3-4ae7-a809-0ee92af864ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (6000, 10240, 2), Shape of y_train: (6000,)\n",
      "Shape of X_test: (1500, 10240, 2), Shape of y_test: (1500,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming data_array and labels are already defined\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(stacked_array, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes of the resulting arrays\n",
    "print(f\"Shape of X_train: {X_train.shape}, Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbe7b855-26d7-49e8-82aa-1c8078a9deec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "428c9837-6838-4320-ad85-c2ea87c080f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c80029fe-a94b-415b-b968-06f9ce8fbe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from aeon.classification import BaseClassifier\n",
    "from aeon.classification.convolution_based._hydra import _SparseScaler\n",
    "from aeon.transformations.collection.convolution_based import MultiRocket\n",
    "from aeon.transformations.collection.convolution_based._hydra import HydraTransformer\n",
    "\n",
    "\n",
    "class MultiRocketHydraClassifier(BaseClassifier):\n",
    "    \"\"\"MultiRocket-Hydra Classifier.\n",
    "\n",
    "    A combination of the Hydra and MultiRocket algorithms. The algorithm concatenates\n",
    "    the output of both algorithms and trains a linear classifier on the combined\n",
    "    features.\n",
    "\n",
    "    See both individual classifier/transformation for more details.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_kernels : int, default=8\n",
    "        Number of kernels per group for the Hydra transform.\n",
    "    n_groups : int, default=64\n",
    "        Number of groups per dilation for the Hydra transform.\n",
    "    class_weight{“balanced”, “balanced_subsample”}, dict or list of dicts, default=None\n",
    "        From sklearn documentation:\n",
    "        If not given, all classes are supposed to have weight one.\n",
    "        The “balanced” mode uses the values of y to automatically adjust weights\n",
    "        inversely proportional to class frequencies in the input data as\n",
    "        n_samples / (n_classes * np.bincount(y))\n",
    "        The “balanced_subsample” mode is the same as “balanced” except that weights\n",
    "        are computed based on the bootstrap sample for every tree grown.\n",
    "        For multi-output, the weights of each column of y will be multiplied.\n",
    "        Note that these weights will be multiplied with sample_weight (passed through\n",
    "        the fit method) if sample_weight is specified.\n",
    "    n_jobs : int, default=1\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        ``-1`` means using all processors.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        If `int`, random_state is the seed used by the random number generator;\n",
    "        If `RandomState` instance, random_state is the random number generator;\n",
    "        If `None`, the random number generator is the `RandomState` instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_classes_ : int\n",
    "        Number of classes. Extracted from the data.\n",
    "    classes_ : ndarray of shape (n_classes_)\n",
    "        Holds the label for each class.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    HydraClassifier\n",
    "    RocketClassifier\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Dempster, A., Schmidt, D.F. and Webb, G.I., 2023. Hydra: Competing\n",
    "        convolutional kernels for fast and accurate time series classification.\n",
    "        Data Mining and Knowledge Discovery, pp.1-27.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from aeon.classification.convolution_based import MultiRocketHydraClassifier\n",
    "    >>> from aeon.testing.data_generation import make_example_3d_numpy\n",
    "    >>> X, y = make_example_3d_numpy(n_cases=10, n_channels=1, n_timepoints=12,\n",
    "    ...                              random_state=0)\n",
    "    >>> clf = MultiRocketHydraClassifier(random_state=0)  # doctest: +SKIP\n",
    "    >>> clf.fit(X, y)  # doctest: +SKIP\n",
    "    MultiRocketHydraClassifier(random_state=0)\n",
    "    >>> clf.predict(X)  # doctest: +SKIP\n",
    "    array([0, 1, 0, 1, 0, 0, 1, 1, 1, 0])\n",
    "    \"\"\"\n",
    "\n",
    "    _tags = {\n",
    "        \"capability:multivariate\": True,\n",
    "        \"capability:multithreading\": True,\n",
    "        \"algorithm_type\": \"convolution\",\n",
    "        \"python_dependencies\": \"torch\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, n_kernels=8, n_groups=64, class_weight=None, n_jobs=1, random_state=None\n",
    "    ):\n",
    "        self.n_kernels = n_kernels\n",
    "        self.n_groups = n_groups\n",
    "        self.class_weight = class_weight\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        self._transform_hydra = HydraTransformer(\n",
    "            n_kernels=self.n_kernels,\n",
    "            n_groups=self.n_groups,\n",
    "            n_jobs=self.n_jobs,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "        Xt_hydra = self._transform_hydra.fit_transform(X)\n",
    "\n",
    "        self._scale_hydra = _SparseScaler()\n",
    "        Xt_hydra = self._scale_hydra.fit_transform(Xt_hydra)\n",
    "\n",
    "        self._transform_multirocket = MultiRocket(\n",
    "            n_jobs=self.n_jobs,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "        Xt_multirocket = self._transform_multirocket.fit_transform(X)\n",
    "\n",
    "        self._scale_multirocket = StandardScaler()\n",
    "        Xt_multirocket = self._scale_multirocket.fit_transform(Xt_multirocket)\n",
    "\n",
    "        Xt = np.concatenate((Xt_hydra, Xt_multirocket), axis=1)\n",
    "\n",
    "        self.classifier = RidgeClassifierCV(\n",
    "            alphas=np.logspace(-3, 3, 10), class_weight=self.class_weight\n",
    "        )\n",
    "        self.classifier.fit(Xt, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict(self, X) -> np.ndarray:\n",
    "        Xt_hydra = self._transform_hydra.transform(X)\n",
    "        Xt_hydra = self._scale_hydra.transform(Xt_hydra)\n",
    "\n",
    "        Xt_multirocket = self._transform_multirocket.transform(X)\n",
    "        Xt_multirocket = self._scale_multirocket.transform(Xt_multirocket)\n",
    "\n",
    "        Xt = np.concatenate((Xt_hydra, Xt_multirocket), axis=1)\n",
    "\n",
    "        return self.classifier.predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43beb2e7-6ce3-45ff-a3dd-3c198f1d9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angus Dempster, Daniel F. Schmidt, Geoffrey I. Webb\n",
    "\n",
    "# HYDRA: Competing convolutional kernels for fast and accurate time series classification\n",
    "# https://arxiv.org/abs/2203.13652\n",
    "\n",
    "# ** EXPERIMENTAL **\n",
    "# This is an *untested*, *experimental* extension of Hydra to multivariate input.\n",
    "\n",
    "# todo: cleanup, documentation\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "class HydraMultivariate(nn.Module):\n",
    "\n",
    "    def __init__(self, input_length, num_channels, k = 8, g = 64, max_num_channels = 8):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.k = k # num kernels per group\n",
    "        self.g = g # num groups\n",
    "\n",
    "        max_exponent = np.log2((input_length - 1) / (9 - 1)) # kernel length = 9\n",
    "\n",
    "        self.dilations = 2 ** torch.arange(int(max_exponent) + 1)\n",
    "        self.num_dilations = len(self.dilations)\n",
    "\n",
    "        self.paddings = torch.div((9 - 1) * self.dilations, 2, rounding_mode = \"floor\").int()\n",
    "\n",
    "        # if g > 1, assign: half the groups to X, half the groups to diff(X)\n",
    "        divisor = 2 if self.g > 1 else 1\n",
    "        _g = g // divisor\n",
    "        self._g = _g\n",
    "\n",
    "        self.W = [self.normalize(torch.randn(divisor, k * _g, 1, 9)) for _ in range(self.num_dilations)]\n",
    "\n",
    "        # combine num_channels // 2 channels (2 < n < max_num_channels)\n",
    "        num_channels_per = np.clip(num_channels // 2, 2, max_num_channels)\n",
    "        self.I = [torch.randint(0, num_channels, (divisor, _g, num_channels_per)) for _ in range(self.num_dilations)]\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(W):\n",
    "        W -= W.mean(-1, keepdims = True)\n",
    "        W /= W.abs().sum(-1, keepdims = True)\n",
    "        return W\n",
    "\n",
    "    # transform in batches of *batch_size*\n",
    "    def batch(self, X, batch_size = 256):\n",
    "        num_examples = X.shape[0]\n",
    "        if num_examples <= batch_size:\n",
    "            return self(X)\n",
    "        else:\n",
    "            Z = []\n",
    "            batches = torch.arange(num_examples).split(batch_size)\n",
    "            for i, batch in enumerate(batches):\n",
    "                Z.append(self(X[batch]))\n",
    "            return torch.cat(Z)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        num_examples = X.shape[0]\n",
    "\n",
    "        if self.g > 1:\n",
    "            diff_X = torch.diff(X)\n",
    "\n",
    "        Z = []\n",
    "\n",
    "        for dilation_index in range(self.num_dilations):\n",
    "\n",
    "            d = self.dilations[dilation_index].item()\n",
    "            p = self.paddings[dilation_index].item()\n",
    "\n",
    "            # diff_index == 0 -> X\n",
    "            # diff_index == 1 -> diff(X)\n",
    "            for diff_index in range(min(2, self.g)):\n",
    "\n",
    "                _Z = F.conv1d(X[:, self.I[dilation_index][diff_index]].sum(2) if diff_index == 0 else diff_X[:, self.I[dilation_index][diff_index]].sum(2),\n",
    "                              self.W[dilation_index][diff_index], dilation = d, padding = p,\n",
    "                              groups = self._g) \\\n",
    "                      .view(num_examples, self._g, self.k, -1)\n",
    "\n",
    "                max_values, max_indices = _Z.max(2)\n",
    "                count_max = torch.zeros(num_examples, self._g, self.k)\n",
    "\n",
    "                min_values, min_indices = _Z.min(2)\n",
    "                count_min = torch.zeros(num_examples, self._g, self.k)\n",
    "\n",
    "                count_max.scatter_add_(-1, max_indices, max_values)\n",
    "                count_min.scatter_add_(-1, min_indices, torch.ones_like(min_values))\n",
    "\n",
    "                Z.append(count_max)\n",
    "                Z.append(count_min)\n",
    "\n",
    "        Z = torch.cat(Z, 1).view(num_examples, -1)\n",
    "\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ddeedb8-6e5a-4c97-a18b-fb0d9aa0f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os  \n",
    "\n",
    "# Add the parent directory to the Python path  \n",
    "sys.path.append(os.path.abspath(os.path.join('..')))  \n",
    "\n",
    "# Now you can import MultiRocket  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce2f7bed-4cf0-4442-9d19-4185baa5f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multirocket.multirocket_multivariate import MultiRocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec5e4b-41f9-43b9-b9e0-6913eb0588a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chang Wei Tan, Angus Dempster, Christoph Bergmeir, Geoffrey I Webb\n",
    "#\n",
    "# MultiRocket: Multiple pooling operators and transformations for fast and effective time series classification\n",
    "# https://arxiv.org/abs/2102.00457\n",
    "import argparse\n",
    "import os\n",
    "import platform\n",
    "import socket\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import pytz\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "\n",
    "from multirocket.multirocket_multivariate import MultiRocket\n",
    "from utils.data_loader import process_ts_data\n",
    "from utils.tools import create_directory\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "itr = 0\n",
    "num_features = 10000\n",
    "save = True\n",
    "num_threads = 0\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-d\", \"--datapath\", type=str, required=False, default=\"E:\\sahithi\")\n",
    "parser.add_argument(\"-p\", \"--problem\", type=str, required=False, default=\"UWaveGestureLibrary\")\n",
    "parser.add_argument(\"-i\", \"--iter\", type=int, required=False, default=0)\n",
    "parser.add_argument(\"-n\", \"--num_features\", type=int, required=False, default=10240)\n",
    "parser.add_argument(\"-t\", \"--num_threads\", type=int, required=False, default=-1)\n",
    "parser.add_argument(\"-s\", \"--save\", type=bool, required=False, default=True)\n",
    "parser.add_argument(\"-v\", \"--verbose\", type=int, required=False, default=2)\n",
    "\n",
    "arguments = parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = arguments.datapath\n",
    "    problem = arguments.problem\n",
    "    num_features = arguments.num_features\n",
    "    num_threads = arguments.num_threads\n",
    "    itr = arguments.iter\n",
    "    save = arguments.save\n",
    "    verbose = arguments.verbose\n",
    "\n",
    "    output_path = os.getcwd() + \"/output/\"\n",
    "    classifier_name = \"MultiRocket_{}\".format(num_features)\n",
    "\n",
    "    data_folder = data_path + problem + \"/\"\n",
    "\n",
    "    if os.path.exists(data_folder):\n",
    "        if num_threads > 0:\n",
    "            numba.set_num_threads(num_threads)\n",
    "        output_path = os.getcwd() + \"/output/\"\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        output_dir = \"{}/multirocket/resample_{}/{}/{}/\".format(\n",
    "            output_path,\n",
    "            itr,\n",
    "            classifier_name,\n",
    "            problem\n",
    "        )\n",
    "        if save:\n",
    "            create_directory(output_dir)\n",
    "\n",
    "        print(\"=======================================================================\")\n",
    "        print(\"Starting Experiments\")\n",
    "        print(\"=======================================================================\")\n",
    "        print(\"Data path: {}\".format(data_path))\n",
    "        print(\"Output Dir: {}\".format(output_dir))\n",
    "        print(\"Iteration: {}\".format(itr))\n",
    "        print(\"Problem: {}\".format(problem))\n",
    "        print(\"Number of Features: {}\".format(num_features))\n",
    "\n",
    "        # set data folder\n",
    "        train_file = data_folder + problem + \"_TRAIN.ts\"\n",
    "        test_file = data_folder + problem + \"_TEST.ts\"\n",
    "\n",
    "        print(\"Loading data\")\n",
    "        X_train, y_train = load_from_tsfile_to_dataframe(train_file)\n",
    "        X_test, y_test = load_from_tsfile_to_dataframe(test_file)\n",
    "\n",
    "        encoder = LabelEncoder()\n",
    "        y_train = encoder.fit_transform(y_train)\n",
    "        y_test = encoder.transform(y_test)\n",
    "\n",
    "        X_train = process_ts_data(X_train, normalise=False)\n",
    "        X_test = process_ts_data(X_test, normalise=False)\n",
    "\n",
    "        nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
    "\n",
    "        classifier = MultiRocket(\n",
    "            num_features=num_features,\n",
    "            classifier=\"logistic\",\n",
    "            verbose=verbose\n",
    "        )\n",
    "        yhat_train = classifier.fit(\n",
    "            X_train, y_train,\n",
    "            predict_on_train=False\n",
    "        )\n",
    "\n",
    "        if yhat_train is not None:\n",
    "            train_acc = accuracy_score(y_train, yhat_train)\n",
    "        else:\n",
    "            train_acc = -1\n",
    "\n",
    "        yhat_test = classifier.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, yhat_test)\n",
    "\n",
    "        # get cpu information\n",
    "        physical_cores = psutil.cpu_count(logical=False)\n",
    "        logical_cores = psutil.cpu_count(logical=True)\n",
    "        cpu_freq = psutil.cpu_freq()\n",
    "        max_freq = cpu_freq.max\n",
    "        min_freq = cpu_freq.min\n",
    "        memory = np.round(psutil.virtual_memory().total / 1e9)\n",
    "\n",
    "        df_metrics = pd.DataFrame(data=np.zeros((1, 21), dtype=np.float), index=[0],\n",
    "                                  columns=['timestamp', 'itr', 'classifier',\n",
    "                                           'num_features',\n",
    "                                           'dataset',\n",
    "                                           'train_acc', 'train_time',\n",
    "                                           'test_acc', 'test_time',\n",
    "                                           'generate_kernel_time',\n",
    "                                           'apply_kernel_on_train_time',\n",
    "                                           'apply_kernel_on_test_time',\n",
    "                                           'train_transform_time',\n",
    "                                           'test_transform_time',\n",
    "                                           'machine', 'processor',\n",
    "                                           'physical_cores',\n",
    "                                           \"logical_cores\",\n",
    "                                           'max_freq', 'min_freq', 'memory'])\n",
    "        df_metrics[\"timestamp\"] = datetime.utcnow().replace(tzinfo=pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        df_metrics[\"itr\"] = itr\n",
    "        df_metrics[\"classifier\"] = classifier_name\n",
    "        df_metrics[\"num_features\"] = num_features\n",
    "        df_metrics[\"dataset\"] = problem\n",
    "        df_metrics[\"train_acc\"] = train_acc\n",
    "        df_metrics[\"train_time\"] = classifier.train_duration\n",
    "        df_metrics[\"test_acc\"] = test_acc\n",
    "        df_metrics[\"test_time\"] = classifier.test_duration\n",
    "        df_metrics[\"generate_kernel_time\"] = classifier.generate_kernel_duration\n",
    "        df_metrics[\"apply_kernel_on_train_time\"] = classifier.apply_kernel_on_train_duration\n",
    "        df_metrics[\"apply_kernel_on_test_time\"] = classifier.apply_kernel_on_test_duration\n",
    "        df_metrics[\"train_transform_time\"] = classifier.train_transforms_duration\n",
    "        df_metrics[\"test_transform_time\"] = classifier.test_transforms_duration\n",
    "        df_metrics[\"machine\"] = socket.gethostname()\n",
    "        df_metrics[\"processor\"] = platform.processor()\n",
    "        df_metrics[\"physical_cores\"] = physical_cores\n",
    "        df_metrics[\"logical_cores\"] = logical_cores\n",
    "        df_metrics[\"max_freq\"] = max_freq\n",
    "        df_metrics[\"min_freq\"] = min_freq\n",
    "        df_metrics[\"memory\"] = memory\n",
    "\n",
    "        print(df_metrics)\n",
    "        if save:\n",
    "            df_metrics.to_csv(output_dir + 'results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8803b3-462b-42f0-bd7a-76f8c975d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming the MultiRocket class is defined similarly to HydraMultivariate\n",
    "class MultiRocketMultivariate(nn.Module):\n",
    "    # Implementation of the MultiRocket model\n",
    "    # Placeholder for your actual implementation\n",
    "    def forward(self, X):\n",
    "        # Extract features (dummy implementation)\n",
    "        return torch.rand(X.shape[0], 128)  # Replace with actual feature extraction\n",
    "\n",
    "# Define your datasets\n",
    "X_train = torch.rand(6000, 10240, 2)  # Example input\n",
    "y_train = np.random.randint(0, 2, 6000)  # Binary classification example\n",
    "X_test = torch.rand(1500, 10240, 2)\n",
    "\n",
    "# Initialize models\n",
    "hydra_model = HydraMultivariate(input_length=10240, num_channels=2)\n",
    "multirocket_model = MultiRocketMultivariate()\n",
    "\n",
    "# Extract features using the Hydra model\n",
    "hydra_features_train = hydra_model(X_train).detach().numpy()\n",
    "hydra_features_test = hydra_model(X_test).detach().numpy()\n",
    "\n",
    "# Extract features using the MultiRocket model\n",
    "multirocket_features_train = multirocket_model(X_train).detach().numpy()\n",
    "multirocket_features_test = multirocket_model(X_test).detach().numpy()\n",
    "\n",
    "# Concatenate features\n",
    "X_train_combined = np.concatenate((hydra_features_train, multirocket_features_train), axis=1)\n",
    "X_test_combined = np.concatenate((hydra_features_test, multirocket_features_test), axis=1)\n",
    "\n",
    "# Train SVM classifier\n",
    "svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear', random_state=0))\n",
    "svm_model.fit(X_train_combined, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test_combined)\n",
    "\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6870976-4595-471d-9ebc-bf51d278926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have the following true and predicted labels\n",
    "true_labels = y_test\n",
    "predicted_labels = y_pred\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a9874-c706-4cf4-8994-2a8642ee01ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56ab6b-14fb-49fd-bb93-2871f970af75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd1e2b-6993-4d25-96b1-e9b690c9ef41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a5fd4-c841-4905-8447-1526a91bc44b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb90cd3-09a6-4571-b807-5f30e06e0792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75754cce-1582-4b12-9c9c-97c5280ac76e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

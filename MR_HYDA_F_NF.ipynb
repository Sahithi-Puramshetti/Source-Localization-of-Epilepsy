{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812f927e-cc28-4494-9a46-5edc409d5b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 10240, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing the files\n",
    "directory = r'E:\\sahithi\\FOCAL'  # Replace with your actual directory path\n",
    "\n",
    "# Initialize an empty array to store the data\n",
    "data_array = np.empty((3750, 10240, 2))  # Assuming 3750 files, each with 10240 time points and 2 columns\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for i, filename in enumerate(os.listdir(directory)):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read data from file using pandas, load both columns (assuming they are separated by commas)\n",
    "        data_df = pd.read_csv(file_path, delimiter=',', usecols=[0], header=None)\n",
    "        \n",
    "        # Convert the data to numpy array\n",
    "        data = data_df.values\n",
    "        \n",
    "        # Ensure the data shape matches expectations\n",
    "        assert data.shape == (10240, 2), f\"File {filename} has unexpected shape {data.shape}\"\n",
    "        \n",
    "        # Store data in the array\n",
    "        data_array[i, :, :] = data\n",
    "\n",
    "# Print the shape of the final array\n",
    "print(data_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c90753-8729-4ff6-a990-f191c5644289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 10240, 2)\n"
     ]
    }
   ],
   "source": [
    "# Find rows that contain any NaN values\n",
    "nan_rows = np.any(np.isnan(data_array), axis=1)  # Rows containing any NaN values\n",
    "\n",
    "# Get the indices of rows with NaN values\n",
    "nan_row_indices = np.where(nan_rows)[0]\n",
    "\n",
    "# Delete the rows that contain NaN values\n",
    "focal_array = np.delete(data_array, nan_row_indices, axis=0)\n",
    "\n",
    "# Print the shape of the resulting array\n",
    "print(focal_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879d8152-c165-47dd-a10e-221b47d61c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_row_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3b1838-7fed-4494-b819-ff22e8d8694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 10240, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing the files\n",
    "directory = r'E:\\sahithi\\NON_FOCAL'  # Replace with your actual directory path\n",
    "\n",
    "# Initialize an empty array to store the data\n",
    "data_arrayy = np.empty((3750, 10240, 2))  # Assuming 3750 files, each with 10240 time points and 2 columns\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for i, filename in enumerate(os.listdir(directory)):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read data from file using pandas, load both columns (assuming they are separated by commas)\n",
    "        data_df = pd.read_csv(file_path, delimiter=',', usecols=[0, 1], header=None)\n",
    "        \n",
    "        # Convert the data to numpy array\n",
    "        data = data_df.values\n",
    "        \n",
    "        # Ensure the data shape matches expectations\n",
    "        assert data.shape == (10240, 2), f\"File {filename} has unexpected shape {data.shape}\"\n",
    "        \n",
    "        # Store data in the array\n",
    "        data_arrayy[i, :, :] = data\n",
    "\n",
    "# Print the shape of the final array\n",
    "print(data_arrayy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c604efa-0a48-4a75-a875-01a789908cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 10240, 2)\n"
     ]
    }
   ],
   "source": [
    "# Find rows that contain any NaN values\n",
    "nan_rows = np.any(np.isnan(data_arrayy), axis=1)  # Rows containing any NaN values\n",
    "\n",
    "# Get the indices of rows with NaN values\n",
    "nan_row_indices = np.where(nan_rows)[0]\n",
    "\n",
    "# Delete the rows that contain NaN values\n",
    "non_focal_array = np.delete(data_arrayy, nan_row_indices, axis=0)\n",
    "\n",
    "# Print the shape of the resulting array\n",
    "print(non_focal_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a5435ba-7492-462b-b3b6-fd0423733389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an array with zeros and ones\n",
    "y_nf = np.ones(3750)\n",
    "print(y_nf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8fbda9-f211-4bab-980e-1df19a049b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an array with zeros and ones\n",
    "y_f = np.zeros(3750)\n",
    "print(y_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea88b927-4069-452c-a928-04726a33b0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked array horizontally:\n",
      "[[[-23.584467  22.621477]\n",
      "  [-20.180197  26.547081]\n",
      "  [-16.631811  28.431639]\n",
      "  ...\n",
      "  [ 17.817694  -4.722577]\n",
      "  [ 17.604982  -3.035015]\n",
      "  [ 15.054106  -2.973314]]\n",
      "\n",
      " [[  1.770672   2.16606 ]\n",
      "  [  7.448948  11.830342]\n",
      "  [ 13.643579  22.593559]\n",
      "  ...\n",
      "  [-12.643819 -14.031724]\n",
      "  [-11.053504 -20.490677]\n",
      "  [-11.453481 -23.759109]]\n",
      "\n",
      " [[-22.218569  -6.315594]\n",
      "  [-20.903902   2.320192]\n",
      "  [-22.920767   8.273545]\n",
      "  ...\n",
      "  [-41.046448 -12.789649]\n",
      "  [-42.06147  -15.303322]\n",
      "  [-46.215527 -21.942698]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-16.280914 -10.494535]\n",
      "  [-14.578059  -6.535292]\n",
      "  [-12.626638  -4.157788]\n",
      "  ...\n",
      "  [-21.634005  -9.552231]\n",
      "  [-22.678474 -10.085828]\n",
      "  [-22.286791  -9.175525]]\n",
      "\n",
      " [[ 11.63331   -1.116699]\n",
      "  [  8.432248  -9.232866]\n",
      "  [ -5.501138 -22.984268]\n",
      "  ...\n",
      "  [ 29.681234 -21.715002]\n",
      "  [ 28.138624 -61.248924]\n",
      "  [ 22.855257 -78.836479]]\n",
      "\n",
      " [[-21.539114 -24.517914]\n",
      "  [-21.258053 -26.164896]\n",
      "  [-23.109068 -29.641247]\n",
      "  ...\n",
      "  [ -1.207698  -4.656437]\n",
      "  [ -2.204289  -4.593308]\n",
      "  [ -3.187739  -4.21675 ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have two arrays\n",
    "array1 = focal_array\n",
    "array2 = non_focal_array\n",
    "\n",
    "# Stack them horizontally\n",
    "stacked_array = np.vstack((array1, array2))\n",
    "\n",
    "print(\"Stacked array horizontally:\")\n",
    "print(stacked_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe347ef7-5f19-4fb0-8b25-7cd1e22ba967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 10240, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f665fe23-2459-48be-b308-a3f76971e3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked array horizontally:\n",
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have two arrays\n",
    "array1 = y_f\n",
    "array2 = y_nf\n",
    "\n",
    "# Stack them horizontally\n",
    "y_labels = np.hstack((array1, array2))\n",
    "\n",
    "print(\"Stacked array horizontally:\")\n",
    "print(y_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c12bfe-a136-44e5-842d-9f4907441ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500,)\n"
     ]
    }
   ],
   "source": [
    "print(y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1faafa1-0c71-462f-8cbb-eaca3e484682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels[3750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14c42e2f-d8d3-4ae7-a809-0ee92af864ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (6000, 10240, 2), Shape of y_train: (6000,)\n",
      "Shape of X_test: (1500, 10240, 2), Shape of y_test: (1500,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming data_array and labels are already defined\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(stacked_array, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes of the resulting arrays\n",
    "print(f\"Shape of X_train: {X_train.shape}, Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbe7b855-26d7-49e8-82aa-1c8078a9deec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c80029fe-a94b-415b-b968-06f9ce8fbe2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\array\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblockwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0matop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblockwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\array\\backends.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m from dask.array.dispatch import (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\array\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_compat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_Recurser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslicing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreplace_ellipsis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetitem_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m from dask.base import (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\array\\slicing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhighlevelgraph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHighLevelGraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcached_cumsum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_arraylike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'cached_cumsum' from 'dask.utils' (C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\dask\\utils.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackends\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrolling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     from dask.dataframe.core import (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\backends.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpercentile_lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\array\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    270\u001b[0m     )\n\u001b[1;32m--> 271\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'cached_cumsum' from 'dask.utils' (C:\\Users\\WORKSTATIONS\\anaconda3\\lib\\site-packages\\dask\\utils.py)\n\nDask array requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                 # either conda install\n  python -m pip install \"dask[array]\" --upgrade  # or python -m pip install",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13144\\3596274662.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolution_based\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hydra\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_SparseScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolution_based\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiRocket\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolution_based\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hydra\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHydraTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\classification\\convolution_based\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolution_based\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arsenal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArsenal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolution_based\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hydra\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHydraClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolution_based\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mr_hydra\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiRocketHydraClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\classification\\convolution_based\\_arsenal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_clone_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m from aeon.transformations.collection.convolution_based import (\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mMiniRocket\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mMiniRocketMultivariate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\transformations\\collection\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m ]\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m from aeon.transformations.collection._collection_wrapper import (\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mCollectionToSeriesWrapper\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\transformations\\collection\\_collection_wrapper.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_clone_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseCollectionTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\transformations\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatatypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert_to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatatypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_series_as_panel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert_to_scitype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatatypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vec_df\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_VectorizedDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\datatypes\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"Module exports: data type definitions, checks, validation, fixtures, converters.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m from aeon.datatypes._check import (\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcheck_is_mtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcheck_is_scitype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\datatypes\\_check.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatatypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hierarchical\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_dict_Hierarchical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatatypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_panel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_dict_Panel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatatypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proba\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_dict_Proba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\datatypes\\_hierarchical\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mconvert_dict\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mconvert_dict_Hierarchical\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[1;33m from aeon.datatypes._hierarchical._examples import (\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mexample_dict\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexample_dict_Hierarchical\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\datatypes\\_hierarchical\\_examples.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0maeon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdask_converters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert_pandas_to_dask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     df_dask = convert_pandas_to_dask(\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0mexample_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pd_multiindex_hier\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Hierarchical\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpartitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aeon\\utils\\conversion\\dask_converters.py\u001b[0m in \u001b[0;36mconvert_pandas_to_dask\u001b[1;34m(obj, npartitions, chunksize, sort)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mother\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0mnames\u001b[0m \u001b[0mare\u001b[0m \u001b[0midentical\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthose\u001b[0m \u001b[0mof\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdask_mi_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;34m'  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     )\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from aeon.classification import BaseClassifier\n",
    "from aeon.classification.convolution_based._hydra import _SparseScaler\n",
    "from aeon.transformations.collection.convolution_based import MultiRocket\n",
    "from aeon.transformations.collection.convolution_based._hydra import HydraTransformer\n",
    "\n",
    "\n",
    "class MultiRocketHydraClassifier(BaseClassifier):\n",
    "    \"\"\"MultiRocket-Hydra Classifier.\n",
    "\n",
    "    A combination of the Hydra and MultiRocket algorithms. The algorithm concatenates\n",
    "    the output of both algorithms and trains a linear classifier on the combined\n",
    "    features.\n",
    "\n",
    "    See both individual classifier/transformation for more details.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_kernels : int, default=8\n",
    "        Number of kernels per group for the Hydra transform.\n",
    "    n_groups : int, default=64\n",
    "        Number of groups per dilation for the Hydra transform.\n",
    "    class_weight{“balanced”, “balanced_subsample”}, dict or list of dicts, default=None\n",
    "        From sklearn documentation:\n",
    "        If not given, all classes are supposed to have weight one.\n",
    "        The “balanced” mode uses the values of y to automatically adjust weights\n",
    "        inversely proportional to class frequencies in the input data as\n",
    "        n_samples / (n_classes * np.bincount(y))\n",
    "        The “balanced_subsample” mode is the same as “balanced” except that weights\n",
    "        are computed based on the bootstrap sample for every tree grown.\n",
    "        For multi-output, the weights of each column of y will be multiplied.\n",
    "        Note that these weights will be multiplied with sample_weight (passed through\n",
    "        the fit method) if sample_weight is specified.\n",
    "    n_jobs : int, default=1\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        ``-1`` means using all processors.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        If `int`, random_state is the seed used by the random number generator;\n",
    "        If `RandomState` instance, random_state is the random number generator;\n",
    "        If `None`, the random number generator is the `RandomState` instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_classes_ : int\n",
    "        Number of classes. Extracted from the data.\n",
    "    classes_ : ndarray of shape (n_classes_)\n",
    "        Holds the label for each class.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    HydraClassifier\n",
    "    RocketClassifier\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Dempster, A., Schmidt, D.F. and Webb, G.I., 2023. Hydra: Competing\n",
    "        convolutional kernels for fast and accurate time series classification.\n",
    "        Data Mining and Knowledge Discovery, pp.1-27.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from aeon.classification.convolution_based import MultiRocketHydraClassifier\n",
    "    >>> from aeon.testing.data_generation import make_example_3d_numpy\n",
    "    >>> X, y = make_example_3d_numpy(n_cases=10, n_channels=1, n_timepoints=12,\n",
    "    ...                              random_state=0)\n",
    "    >>> clf = MultiRocketHydraClassifier(random_state=0)  # doctest: +SKIP\n",
    "    >>> clf.fit(X, y)  # doctest: +SKIP\n",
    "    MultiRocketHydraClassifier(random_state=0)\n",
    "    >>> clf.predict(X)  # doctest: +SKIP\n",
    "    array([0, 1, 0, 1, 0, 0, 1, 1, 1, 0])\n",
    "    \"\"\"\n",
    "\n",
    "    _tags = {\n",
    "        \"capability:multivariate\": True,\n",
    "        \"capability:multithreading\": True,\n",
    "        \"algorithm_type\": \"convolution\",\n",
    "        \"python_dependencies\": \"torch\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, n_kernels=8, n_groups=64, class_weight=None, n_jobs=1, random_state=None\n",
    "    ):\n",
    "        self.n_kernels = n_kernels\n",
    "        self.n_groups = n_groups\n",
    "        self.class_weight = class_weight\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def _fit(self, X, y):\n",
    "        self._transform_hydra = HydraTransformer(\n",
    "            n_kernels=self.n_kernels,\n",
    "            n_groups=self.n_groups,\n",
    "            n_jobs=self.n_jobs,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "        Xt_hydra = self._transform_hydra.fit_transform(X)\n",
    "\n",
    "        self._scale_hydra = _SparseScaler()\n",
    "        Xt_hydra = self._scale_hydra.fit_transform(Xt_hydra)\n",
    "\n",
    "        self._transform_multirocket = MultiRocket(\n",
    "            n_jobs=self.n_jobs,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "        Xt_multirocket = self._transform_multirocket.fit_transform(X)\n",
    "\n",
    "        self._scale_multirocket = StandardScaler()\n",
    "        Xt_multirocket = self._scale_multirocket.fit_transform(Xt_multirocket)\n",
    "\n",
    "        Xt = np.concatenate((Xt_hydra, Xt_multirocket), axis=1)\n",
    "\n",
    "        self.classifier = RidgeClassifierCV(\n",
    "            alphas=np.logspace(-3, 3, 10), class_weight=self.class_weight\n",
    "        )\n",
    "        self.classifier.fit(Xt, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict(self, X) -> np.ndarray:\n",
    "        Xt_hydra = self._transform_hydra.transform(X)\n",
    "        Xt_hydra = self._scale_hydra.transform(Xt_hydra)\n",
    "\n",
    "        Xt_multirocket = self._transform_multirocket.transform(X)\n",
    "        Xt_multirocket = self._scale_multirocket.transform(Xt_multirocket)\n",
    "\n",
    "        Xt = np.concatenate((Xt_hydra, Xt_multirocket), axis=1)\n",
    "\n",
    "        return self.classifier.predict(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba57ffc-9dca-45c6-868b-47a576e31c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultiRocketHydraClassifier(random_state=0)  # doctest: +SKIP\n",
    "clf.fit(X_train,y_train)  # doctest: +SKIP\n",
    "y = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6870976-4595-471d-9ebc-bf51d278926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have the following true and predicted labels\n",
    "true_labels = y_test\n",
    "predicted_labels = y\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
